{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN with attribute\n",
    "## Architecture:\n",
    "* <b>image size: (H, W): from (218, 178) to (32, 32)</b>\n",
    "* <b>Generator:</b> 1 FC + 4 deconv layer <br>\n",
    "    [**(data)**+**(attr)**] \n",
    "    -> [FC] -> flatten -> BN1 -> lReLU1  <br>\n",
    "    ->[Decv2] -> BN2 -> lReLU2 <br>\n",
    "    ->[Decv3] -> BN3 -> lReLU3 <br>\n",
    "    ->[Decv4] -> sigmoid<br>\n",
    "* <b>Discriminator:</b>4 conv layer + 1 FC layer <br>\n",
    "    [**(data)**] \n",
    "    ->[Conv1] -> lReLU1 -> <br>\n",
    "    ->[Conv2] -> BN2 -> lReLU2 <br>\n",
    "    ->[Conv3] -> BN3 -> lReLU3 <br>\n",
    "    ->flatten + **(attr)** -> [FC]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.misc\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mb_size = 16\n",
    "num_of_img = 30000#202599\n",
    "Z_dim = 100  # noize dim\n",
    "X_dim = 116412 # 178 * 218 * 3\n",
    "y_dim = 23 # total attr dim\n",
    "h_dim = 128 # last FC hidden layer\n",
    "\n",
    "H_ = 128 \n",
    "W_ = 128\n",
    "attr_dim = 1 # desired sample attr dim\n",
    "\n",
    "### det directory\n",
    "OUTPUT_DIR = 'output_DCGAN_attr/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "* helper function for risizee image\n",
    "* load data & attr\n",
    "* choose choose specific features for observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_crop(x, crop_h, crop_w,\n",
    "                resize_h=64, resize_w=64):\n",
    "    if crop_w is None:\n",
    "        crop_w = crop_h\n",
    "    h, w = x.shape[:2]\n",
    "    j = int(round((h - crop_h)/2.))\n",
    "    i = int(round((w - crop_w)/2.))\n",
    "    return scipy.misc.imresize(\n",
    "        x[j:j+crop_h, i:i+crop_w], [resize_h, resize_w])\n",
    "\n",
    "def transform(image, input_height, input_width, \n",
    "              resize_height=64, resize_width=64, crop=True):\n",
    "    if crop:\n",
    "        cropped_image = center_crop(\n",
    "          image, input_height, input_width, \n",
    "          resize_height, resize_width)\n",
    "    else:\n",
    "        cropped_image = scipy.misc.imresize(image, [resize_height, resize_width])\n",
    "#     return np.array(cropped_image)/127.5 - 1.\n",
    "    return np.array(cropped_image)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "#     cols = [\"Arched_Eyebrows\", \"Bags_Under_Eyes\", \"Bangs\", \"Big_Lips\", \"Big_Nose\", \"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\",\n",
    "#        \"Bushy_Eyebrows\",\"Eyeglasses\",\"Heavy_Makeup\",\"High_Cheekbones\",\"Male\", \"Mouth_Slightly_Open\",\"Narrow_Eyes\",\n",
    "#        \"No_Beard\",\"Oval_Face\",\"Pointy_Nose\",\"Smiling\",\"Straight_Hair\",\"Wavy_Hair\",\"Wearing_Hat\",\"Young\"]\n",
    "    cols = [\"Male\", \"Smiling\"]\n",
    "\n",
    "    attr = pd.read_csv('data/list_attr_celeba.csv', delim_whitespace=True, skiprows=1, usecols=cols)\n",
    "\n",
    "    attr = attr.values[:num_of_img]\n",
    "    print(\"shape of attr: {}\".format(attr.shape))\n",
    "   \n",
    "    X = []\n",
    "\n",
    "    for i in range(num_of_img):\n",
    "        X_ = scipy.misc.imread('data/img_align_celeba/{:06d}.jpg'.format(i + 1))\n",
    "        X_ = transform(X_, 218, 178, H_, W_, True)\n",
    "        X_ = X_.reshape(H_ * W_ * 3)\n",
    "        #X_ = np.concatenate([X_, attr[i]])\n",
    "        X.append(X_)\n",
    "\n",
    "    X = np.array(X)\n",
    "    \n",
    "   \n",
    "    #print\n",
    "    print(\"shape of one image: {}\".format(X[0].shape))\n",
    "    print(\"min of one image: {}\".format(np.amin(X[0])))\n",
    "    print(\"max of one image: {}\".format(np.amax(X[0])))\n",
    "    print(\"shape of X: {}\".format(X.shape))\n",
    "    \n",
    "    return X, attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]]\n",
      "shape of attr: (30000, 2)"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "###           choose specific features for observation            ###\n",
    "#####################################################################\n",
    "# features = [(\"Arched_Eyebrows\", -1), (\"Bags_Under_Eyes\", -1), (\"Bangs\", 1), (\"Big_Lips\", -1), (\"Big_Nose\", -1),\n",
    "#             (\"Black_Hair\", -1), (\"Blond_Hair\", 1), (\"Brown_Hair\", -1), (\"Bushy_Eyebrows\", -1), (\"Eyeglasses\", -1),\n",
    "#             (\"Heavy_Makeup\", -1), (\"High_Cheekbones\", 1), (\"Male\", 1), (\"Mouth_Slightly_Open\", -1), (\"Narrow_Eyes\", 1),\n",
    "#             (\"No_Beard\", -1), (\"Oval_Face\", 1), (\"Pointy_Nose\", -1), (\"Smiling\", 1), \n",
    "#             (\"Straight_Hair\", 1), (\"Wavy_Hair\", -1), (\"Wearing_Hat\", -1), (\"Young\", 1)]\n",
    "# features = map(lambda x: x[1], features)\n",
    "\n",
    "features = [1, 1] # Male, 1 # Smiling, 1\n",
    "\n",
    "# store sample feature\n",
    "sample_features = np.array(features).reshape((-1,2))\n",
    "print(sample_features)\n",
    "#####################################################################\n",
    "###                        Load celeb data                        ###\n",
    "#####################################################################\n",
    "# (1) reshape date (2) select desired attributes\n",
    "celeb_data, attr = load_train_data()\n",
    "celeb_data = celeb_data.reshape((celeb_data.shape[0], H_, W_, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desired_attr = attr # index 12: Male, index 18: Smiling\n",
    "print \"shape of desired_attr: {}\".format(desired_attr.shape)\n",
    "\n",
    "# store training data and attr\n",
    "train_samples = celeb_data, desired_attr.reshape((-1, 2))\n",
    "\n",
    "\n",
    "# check training data and feature\n",
    "print(\"shape of one image: {}\".format(train_samples[0][0].shape))\n",
    "print(\"feature of the image: {}\".format(train_samples[1][0]))\n",
    "fig = plt.figure()\n",
    "plt.imshow(train_samples[0][0])\n",
    "\n",
    "print(\"shape of one image: {}\".format(train_samples[0][1].shape))\n",
    "print(\"feature of the image: {}\".format(train_samples[1][1]))\n",
    "fig = plt.figure()   \n",
    "plt.imshow(train_samples[0][1])\n",
    "print(\"shape of one image: {}\".format(train_samples[0][2].shape))\n",
    "print(\"feature of the image: {}\".format(train_samples[1][2]))\n",
    "fig = plt.figure()   \n",
    "plt.imshow(train_samples[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement GAN\n",
    "\n",
    "* (1) define layer function\n",
    "* (2) DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viz_grid(Xs, padding):\n",
    "    print Xs.shape\n",
    "    N, H, W, C = Xs.shape\n",
    "    grid_size = int(math.ceil(math.sqrt(N)))\n",
    "    grid_height = H * grid_size + padding * (grid_size + 1)\n",
    "    grid_width = W * grid_size + padding * (grid_size + 1)\n",
    "    grid = np.zeros((grid_height, grid_width, C))\n",
    "    next_idx = 0\n",
    "    y0, y1 = padding, H + padding\n",
    "    for y in range(grid_size):\n",
    "        x0, x1 = padding, W + padding\n",
    "        for x in range(grid_size):\n",
    "            if next_idx < N:\n",
    "                img = Xs[next_idx]\n",
    "                grid[y0:y1, x0:x1] = img\n",
    "                next_idx += 1\n",
    "            x0 += W + padding\n",
    "            x1 += W + padding\n",
    "        y0 += H + padding\n",
    "        y1 += H + padding\n",
    "    return grid\n",
    "\n",
    "def conv2d(input, kernel_size, stride, num_filter, name = 'conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        stride_shape = [1, stride, stride, 1]\n",
    "        filter_shape = [kernel_size, kernel_size, input.get_shape()[3], num_filter]\n",
    "\n",
    "        W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [1, 1, 1, num_filter], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.nn.conv2d(input, W, stride_shape, padding = 'SAME') + b\n",
    "\n",
    "def conv2d_transpose(input, kernel_size, stride, num_filter, name = 'conv2d_transpose'):\n",
    "    with tf.variable_scope(name):\n",
    "        stride_shape = [1, stride, stride, 1]\n",
    "        filter_shape = [kernel_size, kernel_size, num_filter, input.get_shape()[3]]\n",
    "        output_shape = tf.stack([tf.shape(input)[0], tf.shape(input)[1] * 2, tf.shape(input)[2] * 2, num_filter])\n",
    "\n",
    "        W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [1, 1, 1, num_filter], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.nn.conv2d_transpose(input, W, output_shape, stride_shape, padding = 'SAME') + b\n",
    "\n",
    "def fc(input, num_output, name = 'fc'):\n",
    "    with tf.variable_scope(name):\n",
    "        num_input = input.get_shape()[1]\n",
    "        W = tf.get_variable('w', [num_input, num_output], tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [num_output], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.matmul(input, W) + b\n",
    "\n",
    "def batch_norm(input, is_training):\n",
    "    out = tf.contrib.layers.batch_norm(input, decay = 0.99, center = True, scale = True,\n",
    "                                       is_training = is_training, updates_collections = None)\n",
    "    return out\n",
    "\n",
    "def leaky_relu(input, alpha = 0.2):\n",
    "    return tf.maximum(alpha * input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 1000\n",
    "        self.batch_size = 32\n",
    "        self.log_step = 50\n",
    "        self.visualize_step = 200\n",
    "        self.code_size = 100 #64\n",
    "        self.attr_size = 2\n",
    "        self.learning_rate = 2e-4\n",
    "        self.vis_learning_rate = 1e-2\n",
    "        self.recon_steps = 100\n",
    "        self.actmax_steps = 100\n",
    "        \n",
    "        self._dis_called = False\n",
    "        self._gen_called = False\n",
    "    \n",
    "        self.tracked_noise = np.random.normal(0, 1, [64, self.code_size])\n",
    "\n",
    "        self.real_input = tf.placeholder(tf.float32, [None, H_, W_, 3])\n",
    "        self.real_label = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.fake_label = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.noise = tf.placeholder(tf.float32, [None, self.code_size])\n",
    "        self.attr = tf.placeholder(tf.float32, [None, self.attr_size])\n",
    "        self.mismatch_attr = tf.placeholder(tf.float32, [None, self.attr_size])\n",
    "        \n",
    "        \n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.recon_sample = tf.placeholder(tf.float32, [1, 128, 128, 3])\n",
    "        self.actmax_label = tf.placeholder(tf.float32, [1, 1])\n",
    "        \n",
    "        with tf.variable_scope('actmax'):\n",
    "            self.actmax_attr = tf.placeholder(tf.float32, [1, self.attr_size])\n",
    "#             tf.get_variable('actmax_code', [1, self.attr_size])\n",
    "            self.actmax_code = tf.get_variable('actmax_code', [1, self.code_size],\n",
    "                                               initializer = tf.constant_initializer(0.0))\n",
    "        \n",
    "        self._init_ops()\n",
    "\n",
    "    def _discriminator(self, inputs, attr_):\n",
    "        # We have multiple instances of the discriminator in the same computation graph,\n",
    "        # so set variable sharing if this is not the first invocation of this function.\n",
    "        with tf.variable_scope('dis', reuse = self._dis_called):\n",
    "            self._dis_called = True\n",
    "            print inputs\n",
    "            attr_ = tf.tile(tf.expand_dims(tf.tile(tf.expand_dims(attr_, 1), [1, 128, 1]), 1), [1, 128, 1, 1])\n",
    "            print \"attr_: {}\".format(attr_)\n",
    "            inputs = tf.concat([inputs, attr_], axis = 3)\n",
    "            print \"inputs: {}\".format(inputs)\n",
    "            dis_conv1 = conv2d(inputs, 4, 2, 32, 'conv1')\n",
    "#             print dis_conv1.shape\n",
    "            dis_lrelu1 = leaky_relu(dis_conv1)\n",
    "            dis_conv2 = conv2d(dis_lrelu1, 4, 2, 64, 'conv2')\n",
    "#             print dis_conv2.shape\n",
    "            dis_batchnorm2 = batch_norm(dis_conv2, self.is_train)\n",
    "            dis_lrelu2 = leaky_relu(dis_batchnorm2)\n",
    "            dis_conv3 = conv2d(dis_lrelu2, 4, 2, 128, 'conv3')\n",
    "#             print dis_conv3.shape\n",
    "            dis_batchnorm3 = batch_norm(dis_conv3, self.is_train)\n",
    "            dis_lrelu3 = leaky_relu(dis_batchnorm3)\n",
    "            dis_reshape3 = tf.reshape(dis_lrelu3, [-1, 16 * 16 * 128]) # 16 * 16 * 128\n",
    "#             print dis_reshape3.shape\n",
    "            # dis_addAttr3 = tf.concat([dis_reshape3, attr_], axis = 1)\n",
    "            dis_fc4 = fc(dis_reshape3, 1, 'fc4')\n",
    "            dis_prob = tf.nn.sigmoid(dis_fc4)\n",
    "            return tf.reduce_mean(dis_prob), dis_fc4\n",
    "\n",
    "    def _generator(self, noise, attr_):\n",
    "        with tf.variable_scope('gen', reuse = self._gen_called):\n",
    "            self._gen_called = True\n",
    "            inputs = tf.concat(values = [noise, attr_], axis = 1)\n",
    "            gen_fc1 = fc(inputs, 4 * 4 * 128 * 16, 'fc1')\n",
    "            gen_reshape1 = tf.reshape(gen_fc1, [-1, 16, 16, 128])\n",
    "#             print gen_reshape1.shape\n",
    "            gen_batchnorm1 = batch_norm(gen_reshape1, self.is_train)\n",
    "            gen_lrelu1 = leaky_relu(gen_batchnorm1)\n",
    "            gen_conv2 = conv2d_transpose(gen_lrelu1, 4, 2, 64, 'conv2')\n",
    "#             print gen_conv2.shape\n",
    "            gen_batchnorm2 = batch_norm(gen_conv2, self.is_train)\n",
    "            gen_lrelu2 = leaky_relu(gen_batchnorm2)\n",
    "            gen_conv3 = conv2d_transpose(gen_lrelu2, 4, 2, 32, 'conv3')\n",
    "#             print gen_conv3.shape\n",
    "            gen_batchnorm3 = batch_norm(gen_conv3, self.is_train)\n",
    "            gen_lrelu3 = leaky_relu(gen_batchnorm3)\n",
    "            gen_conv4 = conv2d_transpose(gen_lrelu3, 4, 2, 3, 'conv4')\n",
    "#             print gen_conv4.shape\n",
    "            gen_sigmoid4 = tf.sigmoid(gen_conv4)\n",
    "            return gen_sigmoid4\n",
    "\n",
    "    def _loss(self, labels, logits):\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def _reconstruction_loss(self, generated, target):\n",
    "        loss = tf.nn.l2_loss(generated - target)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    # Define operations\n",
    "    def _init_ops(self):\n",
    "        \n",
    "        self.fake_samples_op = self._generator(self.noise, self.attr)\n",
    "        self.D_fake_prob, D_fake = self._discriminator(self.fake_samples_op, self.attr)\n",
    "        self.D_real_prob, D_real = self._discriminator(self.real_input, self.attr)\n",
    "        self.D_mis_prob, D_mismatch = self._discriminator(self.real_input, self.mismatch_attr)\n",
    "        \n",
    "        self.dis_loss_op = self._loss(self.real_label, D_real) + self._loss(self.fake_label, D_fake) + self._loss(self.fake_label, D_mismatch)\n",
    "        self.gen_loss_op = self._loss(self.real_label, D_fake)\n",
    "        \n",
    "        all_vars = tf.trainable_variables()\n",
    "\n",
    "        d_var = [v for v in all_vars if v.name.startswith('dis')]\n",
    "        g_var = [v for v in all_vars if v.name.startswith('gen')]\n",
    "        \n",
    "        # print the var status to check var\n",
    "        import tensorflow.contrib.slim as slim\n",
    "        print(\"********* d_var ********** \")\n",
    "        slim.model_analyzer.analyze_vars(d_var, print_info=True)\n",
    "        print(\"********* g_var ********** \") \n",
    "        slim.model_analyzer.analyze_vars(g_var, print_info=True)\n",
    "\n",
    "        dis_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.5)\n",
    "        self.dis_train_op = dis_optimizer.minimize(self.dis_loss_op, var_list = d_var)\n",
    "        \n",
    "        gen_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.5)\n",
    "        self.gen_train_op = gen_optimizer.minimize(self.gen_loss_op, var_list = g_var)\n",
    "        \n",
    "#         ##\n",
    "#         self.actmax_sample_op = self._generator(self.actmax_code, self.actmax_attr)\n",
    "#         actmax_dis = self._discriminator(self.actmax_sample_op, self.actmax_attr)\n",
    "#         self.actmax_loss_op = self._loss(self.actmax_label, actmax_dis)\n",
    "\n",
    "#         actmax_optimizer = tf.train.AdamOptimizer(self.vis_learning_rate)\n",
    "#         self.actmax_op = actmax_optimizer.minimize(self.actmax_loss_op, var_list = [self.actmax_code])\n",
    "        \n",
    "#         ##\n",
    "#         self.recon_loss_op = self._reconstruction_loss(self.actmax_sample_op, self.recon_sample)\n",
    "        \n",
    "#         recon_optimizer = tf.train.AdamOptimizer(self.vis_learning_rate)\n",
    "#         self.reconstruct_op = recon_optimizer.minimize(self.recon_loss_op, var_list = [self.actmax_code])\n",
    "    \n",
    "    # Training function\n",
    "    def train(self, sess, train_samples, sample_features):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        train_data, train_attr= train_samples\n",
    "        num_train = train_data.shape[0]\n",
    "        step = 0\n",
    "        \n",
    "        # smooth the loss curve so that it does not fluctuate too much\n",
    "        smooth_factor = 0.95\n",
    "        plot_dis_s = 0\n",
    "        plot_gen_s = 0\n",
    "        plot_ws = 0\n",
    "        \n",
    "        plot_dis_f = 0\n",
    "        plot_dis_r = 0\n",
    "        plot_dis_m = 0\n",
    "        \n",
    "        dis_losses = []\n",
    "        fakes = []\n",
    "        reals = []\n",
    "        mismatchs = []\n",
    "        gen_losses = []\n",
    "        \n",
    "        import copy\n",
    "        update_ratio = 2\n",
    "        gen_loss = 0.0\n",
    "        dis_loss = 0.0\n",
    "        for epoch in range(self.num_epoch):\n",
    "            for i in range(num_train // self.batch_size):\n",
    "                step += 1\n",
    "\n",
    "                batch_data = train_data[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                batch_attr = train_attr[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                noise = np.random.normal(0, 1, [self.batch_size, self.code_size])\n",
    "                zeros = np.zeros([self.batch_size, 1])\n",
    "                ones = np.ones([self.batch_size, 1])\n",
    "                mismatch_attr = copy.deepcopy(batch_attr)\n",
    "                for i in range(len(mismatch_attr)):\n",
    "                    for j in range(len(mismatch_attr[i])):\n",
    "                        \"\"\"\n",
    "                        if i == 0:\n",
    "                            print(\"before\")\n",
    "                            print(mismatch_attr[i][j])\n",
    "                            print(batch_attr[i][j])\n",
    "                        \"\"\"\n",
    "                        if mismatch_attr[i][j] == 0:\n",
    "                            mismatch_attr[i][j] = 1\n",
    "                        else:\n",
    "                            mismatch_attr[i][j] = 0\n",
    "                        \"\"\"\n",
    "                        if i == 0:\n",
    "                            print(\"after\")\n",
    "                            print(mismatch_attr[i][j])\n",
    "                            print(batch_attr[i][j])\n",
    "                        \"\"\"\n",
    "                # print \"attr shape: {} {}\".format(batch_attr.shape, mismatch_attr.shape)\n",
    "                if batch_attr.shape[0] != self.batch_size:\n",
    "                    print \"Error: attr shape not match: attr shape {}, batch size {}\".format(batch_attr.shape, self.batch_size)\n",
    "                    continue\n",
    "                dis_feed_dict = {self.real_input: batch_data, \n",
    "                                 self.noise: noise, \n",
    "                                 self.attr: batch_attr,\n",
    "                                 self.mismatch_attr: mismatch_attr,\n",
    "                                 self.real_label: ones, \n",
    "                                 self.fake_label: zeros, \n",
    "                                 self.is_train:True}\n",
    "                \n",
    "                gen_feed_dict = {self.noise: noise, \n",
    "                                 self.attr: batch_attr,\n",
    "                                 self.real_label: ones, \n",
    "                                 self.is_train: True}\n",
    "                \"\"\"\n",
    "                if i % update_ratio == 0:\n",
    "                    _, dis_loss = sess.run([self.dis_train_op, self.dis_loss_op], feed_dict = dis_feed_dict)\n",
    "                else:\n",
    "                    _, gen_loss = sess.run([self.gen_train_op, self.gen_loss_op], feed_dict = gen_feed_dict)\n",
    "                \"\"\"\n",
    "                _, dis_loss, dis_fake, dis_real, dis_mis = sess.run([self.dis_train_op, self.dis_loss_op, self.D_fake_prob,\n",
    "                                                                    self.D_real_prob, self.D_mis_prob], feed_dict = dis_feed_dict)\n",
    "                _, gen_loss = sess.run([self.gen_train_op, self.gen_loss_op], feed_dict = gen_feed_dict)\n",
    "\n",
    "                plot_dis_s = plot_dis_s * smooth_factor + dis_loss * (1 - smooth_factor)\n",
    "                plot_gen_s = plot_gen_s * smooth_factor + gen_loss * (1 - smooth_factor)\n",
    "                plot_ws = plot_ws * smooth_factor + (1 - smooth_factor)\n",
    "                dis_losses.append(plot_dis_s / plot_ws)\n",
    "                gen_losses.append(plot_gen_s / plot_ws)\n",
    "                \n",
    "                plot_dis_f = plot_dis_f * smooth_factor + dis_fake * (1 - smooth_factor)\n",
    "                plot_dis_r = plot_dis_r * smooth_factor + dis_real * (1 - smooth_factor)\n",
    "                plot_dis_m = plot_dis_m * smooth_factor + dis_mis * (1 - smooth_factor)\n",
    "                fakes.append(plot_dis_f / plot_ws)\n",
    "                reals.append(plot_dis_r / plot_ws)\n",
    "                mismatchs.append(plot_dis_m / plot_ws)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('Iteration {0}: dis loss = {1:.4f}, gen loss = {2:.4f}'.format(step, dis_loss, gen_loss))\n",
    "\n",
    "            if not os.path.exists(OUTPUT_DIR):\n",
    "                os.mkdir(OUTPUT_DIR)\n",
    "                \n",
    "            filename = OUTPUT_DIR + '%d_%d.png' % (epoch,i)\n",
    "            \n",
    "            fig = plt.figure(figsize = (8, 8)) \n",
    "            ax1 = plt.subplot(111)\n",
    "            image = viz_grid(self.generate(self.tracked_noise, sample_features), 1)\n",
    "            scipy.misc.imsave(filename, image)\n",
    "            ax1.imshow(image)\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(dis_losses)\n",
    "            plt.title('discriminator loss')\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(fakes, label=\"fake\", color=\"blue\")\n",
    "            plt.plot(reals, label=\"real\", color=\"red\")\n",
    "            plt.plot(mismatchs, label=\"mismatch\", color=\"green\")\n",
    "            plt.legend(['fake', 'real', 'mismatch'], loc='upper left')\n",
    "            plt.title('D_fake_real_mis')\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('prob')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(gen_losses)\n",
    "            plt.title('generator loss')\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "            \n",
    "            dis_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dis')\n",
    "            gen_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'gen')\n",
    "            saver = tf.train.Saver(dis_var_list + gen_var_list)\n",
    "            saver.save(sess, 'model/dcgan_%d' % epoch)\n",
    "\n",
    "    # Find the reconstruction of one input sample\n",
    "    def reconstruct_one_sample(self, sample):\n",
    "        ##\n",
    "        code = np.random.random([1, self.code_size])\n",
    "        actmax_init_val = tf.convert_to_tensor(code, dtype = tf.float32)\n",
    "        \n",
    "        sess.run(self.actmax_code.assign(actmax_init_val))\n",
    "        last_reconstruction = None\n",
    "        last_loss = None\n",
    "        for i in range(self.recon_steps):\n",
    "        \n",
    "            ##\n",
    "            recon_feed_dict = {self.recon_sample: sample, \n",
    "                               self.is_train: False}\n",
    "            \n",
    "            run_ops = [self.recon_loss_op, self.reconstruct_op, self.actmax_sample_op]\n",
    "            last_loss, _, last_reconstruction = sess.run(run_ops, feed_dict = recon_feed_dict)\n",
    "        return last_loss, last_reconstruction\n",
    "\n",
    "    # Find the reconstruction of a batch of samples\n",
    "    def reconstruct(self, samples):\n",
    "        reconstructions = np.zeros(samples.shape)\n",
    "        total_loss = 0\n",
    "        for i in range(samples.shape[0]):\n",
    "            loss, reconstructions[i:i+1] = self.reconstruct_one_sample(samples[i:i+1])\n",
    "            total_loss += loss\n",
    "        return total_loss / samples.shape[0], reconstructions\n",
    "\n",
    "    # Generates a single sample from input code\n",
    "    def generate_one_sample(self, code, attr_):\n",
    "\n",
    "        gen_vis_feed_dict = {self.noise: code, \n",
    "                             self.attr: attr_, \n",
    "                             self.is_train: False}\n",
    "        \n",
    "        generated = sess.run(self.fake_samples_op, feed_dict = gen_vis_feed_dict)\n",
    "        return generated\n",
    "\n",
    "    # Generates samples from input batch of codes\n",
    "    def generate(self, codes, attr_):\n",
    "        generated = np.zeros((codes.shape[0], H_, W_, 3))\n",
    "        for i in range(codes.shape[0]):\n",
    "            generated[i:i+1] = self.generate_one_sample(codes[i:i+1], attr_)\n",
    "        return generated\n",
    "\n",
    "    # Perform activation maximization on one initial code\n",
    "    def actmax_one_sample(self, initial_code):\n",
    "        \n",
    "        ##\n",
    "        actmax_init_val = tf.convert_to_tensor(initial_code, dtype = tf.float32)\n",
    "        sess.run(self.actmax_code.assign(actmax_init_val))\n",
    "        for i in range(self.actmax_steps):\n",
    "            actmax_feed_dict = {\n",
    "                self.actmax_label: np.ones([1, 1]),\n",
    "                self.is_train: False\n",
    "            }\n",
    "            _, last_actmax = sess.run([self.actmax_op, self.actmax_sample_op], feed_dict = actmax_feed_dict)\n",
    "        return last_actmax\n",
    "\n",
    "    # Perform activation maximization on a batch of different initial codes\n",
    "    def actmax(self, initial_codes):\n",
    "        actmax_results = np.zeros((initial_codes.shape[0], 32, 32, 3))\n",
    "        for i in range(initial_codes.shape[0]):\n",
    "            actmax_results[i:i+1] = self.actmax_one_sample(initial_codes[i:i+1])\n",
    "        return actmax_results.clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "* start to train\n",
    "* save the trained model at 'model/dcgan' \n",
    "* save output image & loss curve at OUTPUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        dcgan = DCGAN()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        dcgan.train(sess, train_samples, sample_features)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
